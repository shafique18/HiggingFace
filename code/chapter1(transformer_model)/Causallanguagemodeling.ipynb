{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54801e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = eli5.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b813f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4bb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = eli5.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])\n",
    "\n",
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d2b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec65c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582027bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dbbf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e4e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6612ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AdamW\n",
    "from transformers import AdamWeightDecay\n",
    "\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForCausalLM\n",
    "\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c92504",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    lm_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    lm_dataset[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94f922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)  # No loss argument!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9886767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from transformers.keras_callbacks import\n",
    "\n",
    "# callback = PushToHubCallback(\n",
    "#     output_dir=\"my_awesome_eli5_clm-model\",\n",
    "#     # tokenizer=tokenizer,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b0fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d04b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(r\"C:\\Users\\shafi\\Documents\\Shafique\\Learning\\HugginFace\\code\\chapter1(transformer_model)\\my_local_model\")\n",
    "tokenizer.save_pretrained(r\"C:\\Users\\shafi\\Documents\\Shafique\\Learning\\HugginFace\\code\\chapter1(transformer_model)\\my_local_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1bcacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Somatic hypermutation allows the immune system to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "425554f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at C:\\Users\\shafi\\Documents\\Shafique\\Learning\\HugginFace\\code\\chapter1(transformer_model)\\my_local_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "model_path = r\"C:\\Users\\shafi\\Documents\\Shafique\\Learning\\HugginFace\\code\\chapter1(transformer_model)\\my_local_model\"\n",
    "model = TFAutoModelForCausalLM.from_pretrained(model_path, from_pt=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f1a462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63fc286d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Somatic hypermutation allows the immune system to \"keep\" the virus from taking over the body. There are many different methods of getting immune cells to behave like normal, if not super-normal, but the vast majority of people are in good'}]\n"
     ]
    }
   ],
   "source": [
    "print(generator(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9483fc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x000002301E62C850>\n"
     ]
    }
   ],
   "source": [
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09189329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
